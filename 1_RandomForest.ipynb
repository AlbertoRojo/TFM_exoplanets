{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da65b15-2150-4479-8f1e-57df0033d37c",
   "metadata": {},
   "source": [
    "Modelo 1: Random Forest + optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5afd3-8033-4704-b360-2d922595ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    return {\n",
    "        'mean_flux': np.mean(flux),\n",
    "        'std_flux': np.std(flux),\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'iqr_flux': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'exoplanet': obj['exoplanet']\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=30, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curva Precisión-Recall')\n",
    "    plt.show()\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "train_evaluate_model(X, y, \"modelo1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee93e4-0626-4cee-93fb-d1d5364173b6",
   "metadata": {},
   "source": [
    "Resultados para modelo1:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'classifier__n_estimators': 400, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 15, 'classifier__class_weight': 'balanced'}\n",
    "\n",
    "Matriz de confusión:<br>\n",
    "[[1971   64]<br>\n",
    " [  57   30]]<br>\n",
    "\n",
    "            Informe de clasificación:\n",
    "              precision    recall  f1-score   support<br>\n",
    "\n",
    "           0       0.97      0.97      0.97      2035\n",
    "           1       0.32      0.34      0.33        87\n",
    "\n",
    "    accuracy                            0.94      2122\n",
    "    macro avg       0.65      0.66      0.65      2122 \n",
    "    weighted avg    0.95      0.94      0.94      2122\n",
    "\n",
    "\n",
    "ROC AUC Score: 0.8138<br>\n",
    "Modelo guardado como: exoplanet_model_tess_flux_only.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e4a65-6460-46f6-a9fb-62224a959592",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d5831-ddbf-4846-add0-bd4aa12ab163",
   "metadata": {},
   "source": [
    "Modelo 2: Random Forest + optimización de hiperparámetros + manejo del desequilibro de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec2862-8af4-4efb-8267-b1913b9aa5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    return {\n",
    "        'mean_flux': np.mean(flux),\n",
    "        'std_flux': np.std(flux),\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'iqr_flux': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'exoplanet': obj['exoplanet']\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=30, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curva Precisión-Recall')\n",
    "    plt.show()\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "train_evaluate_model(X, y, \"model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cde91e-39da-4d72-9066-985347928d5a",
   "metadata": {},
   "source": [
    "Resultados para model2:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'classifier__n_estimators': 300, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None, 'classifier__class_weight': 'balanced'}\n",
    "\n",
    "Matriz de confusión:<br>\n",
    "[[1857  178]<br>\n",
    " [  45   42]]\n",
    "\n",
    "    Informe de clasificación:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.91      0.94      2035\n",
    "           1       0.19      0.48      0.27        87\n",
    "\n",
    "    accuracy                           0.89      2122\n",
    "    macro avg      0.58      0.70      0.61      2122\n",
    "    weighted avg   0.94      0.89      0.92      2122\n",
    "\n",
    "\n",
    "ROC AUC Score: 0.8284"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe611c8d-74e1-495c-9196-14d1684038a4",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d155e-91bc-40ae-9a22-7fb83916c70d",
   "metadata": {},
   "source": [
    "Modelo 3: Random Forest + optimización de hiperparámetros + manejo del desequilibro de clases + feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed0f57-6ed2-4b22-8939-7259d7bebd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    return {\n",
    "        'mean_flux': np.mean(flux),\n",
    "        'std_flux': np.std(flux),\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'iqr_flux': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'mean_flux_err': np.mean(flux_err),\n",
    "        'std_flux_err': np.std(flux_err),\n",
    "        'flux_amplitude': np.max(flux) - np.min(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_max': np.max(flux),\n",
    "        'flux_min': np.min(flux),\n",
    "        'flux_range': np.ptp(flux),\n",
    "        'flux_mad': np.median(np.abs(flux - np.median(flux))),\n",
    "        'flux_energy': np.sum(flux**2),\n",
    "        'flux_1st_quartile': np.percentile(flux, 25),\n",
    "        'flux_3rd_quartile': np.percentile(flux, 75),\n",
    "        'flux_above_mean': np.mean(flux > np.mean(flux)),\n",
    "        'flux_max_to_min_ratio': np.max(flux) / np.min(flux) if np.min(flux) != 0 else np.inf,\n",
    "        'exoplanet': obj['exoplanet']\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=30, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curva Precisión-Recall')\n",
    "    plt.show()\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "train_evaluate_model(X, y, \"model3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2282d7-f843-48c1-b9f4-f7ccac28c8ac",
   "metadata": {},
   "source": [
    "Resultados para model3:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'classifier__n_estimators': 500, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None, 'classifier__class_weight': None}\n",
    "\n",
    "Matriz de confusión:<br>\n",
    "[[1988   47]<br>\n",
    " [  20   67]]\n",
    "\n",
    "    Informe de clasificación:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.98      0.98      2035\n",
    "           1       0.59      0.77      0.67        87\n",
    "\n",
    "    accuracy                           0.97      2122\n",
    "    macro avg      0.79      0.87      0.83      2122\n",
    "    weighted avg   0.97      0.97      0.97      2122\n",
    "\n",
    "\n",
    "ROC AUC Score: 0.9761"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f3d7a-a90f-4a60-84db-74a6e99c73d6",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e336f-c09e-4a59-854f-6034319cf7b1",
   "metadata": {},
   "source": [
    "Modelo 4: Random Forest + optimización de hiperparámetros + manejo del desequilibro de clases + feature engineering avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d2970-0af0-469c-a9e5-ebbc1da896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    features = {\n",
    "        'mean_flux': np.mean(flux),\n",
    "        'std_flux': np.std(flux),\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'iqr_flux': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'mean_flux_err': np.mean(flux_err),\n",
    "        'std_flux_err': np.std(flux_err),\n",
    "        'flux_amplitude': np.max(flux) - np.min(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_max': np.max(flux),\n",
    "        'flux_min': np.min(flux),\n",
    "        'flux_range': np.ptp(flux),\n",
    "        'flux_mad': np.median(np.abs(flux - np.median(flux))),\n",
    "        'flux_energy': np.sum(flux**2),\n",
    "        'flux_1st_quartile': np.percentile(flux, 25),\n",
    "        'flux_3rd_quartile': np.percentile(flux, 75),\n",
    "        'flux_above_mean': np.mean(flux > np.mean(flux)),\n",
    "        'flux_max_to_min_ratio': np.max(flux) / np.min(flux) if np.min(flux) != 0 else np.inf,\n",
    "    }\n",
    "\n",
    "    # Características adicionales\n",
    "    peaks, _ = find_peaks(flux, height=np.mean(flux) + np.std(flux))\n",
    "    valleys, _ = find_peaks(-flux, height=-(np.mean(flux) - np.std(flux)))\n",
    "    features['num_peaks'] = len(peaks)\n",
    "    features['num_valleys'] = len(valleys)\n",
    "    features['peak_valley_ratio'] = features['num_peaks'] / (features['num_valleys'] + 1)\n",
    "    features['peak_ratio'] = len(peaks) / len(flux)\n",
    "    features['valley_ratio'] = len(valleys) / len(flux)\n",
    "    features['peak_valley_ratio'] = features['peak_ratio'] / (features['valley_ratio'] + 1e-10)  # Evitar división por cero\n",
    "\n",
    "    autocorr = np.correlate(flux - np.mean(flux), flux - np.mean(flux), mode='full')\n",
    "    autocorr = autocorr[len(autocorr)//2:]\n",
    "    features['max_autocorr'] = np.max(autocorr[1:])\n",
    "\n",
    "    sorted_flux = np.sort(flux)\n",
    "    features['flux_drop'] = np.mean(sorted_flux[:len(sorted_flux)//10]) - np.mean(flux)\n",
    "\n",
    "    # Características basadas en FFT\n",
    "    fft_vals = np.abs(fft(flux))\n",
    "    features['fft_max'] = np.max(fft_vals[1:])\n",
    "    features['fft_mean'] = np.mean(fft_vals[1:])\n",
    "    features['fft_std'] = np.std(fft_vals[1:])\n",
    "\n",
    "    # Características de forma de onda\n",
    "    zero_crossings = np.where(np.diff(np.sign(flux - np.mean(flux))))[0]\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / len(flux)\n",
    "\n",
    "    features['log_flux_energy'] = np.log1p(features['flux_energy'])\n",
    "\n",
    "    features['exoplanet'] = obj['exoplanet']\n",
    "    return features\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=30, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Análisis adicional de las características más importantes\n",
    "        top_features = feature_importance_df['feature'].head(5).tolist()\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features, 1):\n",
    "            plt.subplot(2, 3, i)\n",
    "            sns.histplot(data=df, x=feature, hue='exoplanet', kde=True)\n",
    "            plt.title(f'Distribución de {feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curva Precisión-Recall')\n",
    "    plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "feature_importance_df = train_evaluate_model(X, y, \"model4\")\n",
    "\n",
    "# Matriz de correlación\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.drop('exoplanet', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Matriz de Correlación de Características')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76681930-0d4a-41b6-aa8d-e5495e5632ee",
   "metadata": {},
   "source": [
    "Resultados para model4:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'classifier__n_estimators': 300, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None, 'classifier__class_weight': 'balanced'}\n",
    "\n",
    "Matriz de confusión:<br>\n",
    "[[2009   26]<br>\n",
    " [  15   72]]\n",
    "\n",
    "    Informe de clasificación:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      2035\n",
    "           1       0.73      0.83      0.78        87\n",
    "\n",
    "    accuracy                           0.98      2122\n",
    "    macro avg      0.86      0.91      0.88      2122\n",
    "    weighted avg   0.98      0.98      0.98      2122\n",
    "\n",
    "\n",
    "ROC AUC Score: 0.9718"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a180d5-e279-463f-9f06-46c06936b619",
   "metadata": {},
   "source": [
    "Modelo 5: Random Forest + optimización de hiperparámetros + manejo del desequilibro de clases + feature engineering avanzado + feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba632-1f9a-41f7-8907-f3392ae10dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    features = {\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_energy': np.sum(flux**2)\n",
    "    }\n",
    "\n",
    "    # Características adicionales\n",
    "    peaks, _ = find_peaks(flux, height=np.mean(flux) + np.std(flux))\n",
    "    valleys, _ = find_peaks(-flux, height=-(np.mean(flux) - np.std(flux)))\n",
    "    # features['num_peaks'] = len(peaks)\n",
    "    # features['num_valleys'] = len(valleys)\n",
    "    features['valley_ratio'] = len(valleys) / len(flux)\n",
    "    # features['peak_valley_ratio'] = (len(peaks) / len(flux))/ (len(valleys) / len(flux) + 1e-10)  # Evitar división por cero\n",
    "\n",
    "\n",
    "    # Características basadas en FFT\n",
    "    fft_vals = np.abs(fft(flux))\n",
    "    features['fft_max'] = np.max(fft_vals[1:])\n",
    "\n",
    "    # Características de forma de onda\n",
    "    zero_crossings = np.where(np.diff(np.sign(flux - np.mean(flux))))[0]\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / len(flux)\n",
    "\n",
    "    features['log_flux_energy'] = np.log1p(features['flux_energy'])\n",
    "\n",
    "    features['exoplanet'] = obj['exoplanet']\n",
    "    return features\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=30, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nInforme de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/content/drive/My Drive/TFM_b/feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Análisis adicional de las características más importantes\n",
    "        top_features = feature_importance_df['feature'].head(5).tolist()\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features, 1):\n",
    "            plt.subplot(2, 3, i)\n",
    "            sns.histplot(data=df, x=feature, hue='exoplanet', kde=True)\n",
    "            plt.title(f'Distribución de {feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curva Precisión-Recall')\n",
    "    plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "feature_importance_df = train_evaluate_model(X, y, \"model5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c943160-9e58-4498-aded-103dfdca5f34",
   "metadata": {},
   "source": [
    "Resultados para model5:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'classifier__n_estimators': 400, 'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None, 'classifier__class_weight': None}\n",
    "\n",
    "Matriz de confusión:<br>\n",
    "[[2018   17]<br>\n",
    " [  19   68]]\n",
    "\n",
    "    Informe de clasificación:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      2035\n",
    "           1       0.80      0.78      0.79        87\n",
    "\n",
    "    accuracy                           0.98      2122\n",
    "    macro avg      0.90      0.89      0.89      2122\n",
    "    weighted avg   0.98      0.98      0.98      2122\n",
    "\n",
    "\n",
    "ROC AUC Score: 0.9663"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112855-ac3a-45ba-90c3-617035b37b0e",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae42ba-b89a-4798-b6f5-36b3ca067321",
   "metadata": {},
   "source": [
    "Modelo 6: Random Forest + optimización de hiperparámetros + manejo del desequilibro de clases + feature engineering avanzado + feature selection + regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163d3e0-c0f9-4ce4-a00e-d37e7fde89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "with open('/content/drive/My Drive/TFM_b/tess_combined_training_data.pkl', 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "def extract_features(obj):\n",
    "    flux = np.array(obj['norm_flux'])\n",
    "    flux_err = np.array(obj['norm_flux_err'])\n",
    "\n",
    "    features = {\n",
    "        'skew_flux': stats.skew(flux),\n",
    "        'kurt_flux': stats.kurtosis(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_energy': np.sum(flux**2)\n",
    "    }\n",
    "\n",
    "    # Características adicionales\n",
    "    peaks, _ = find_peaks(flux, height=np.mean(flux) + np.std(flux))\n",
    "    valleys, _ = find_peaks(-flux, height=-(np.mean(flux) - np.std(flux)))\n",
    "    features['valley_ratio'] = len(valleys) / len(flux)\n",
    "\n",
    "    # Características basadas en FFT\n",
    "    fft_vals = np.abs(fft(flux))\n",
    "    features['fft_max'] = np.max(fft_vals[1:])\n",
    "\n",
    "    # Características de forma de onda\n",
    "    zero_crossings = np.where(np.diff(np.sign(flux - np.mean(flux))))[0]\n",
    "    features['zero_crossing_rate'] = len(zero_crossings) / len(flux)\n",
    "\n",
    "    features['log_flux_energy'] = np.log1p(features['flux_energy'])\n",
    "\n",
    "    features['exoplanet'] = obj['exoplanet']\n",
    "    return features\n",
    "\n",
    "df = pd.DataFrame([extract_features(obj) for obj in combined_data])\n",
    "\n",
    "def train_evaluate_model(X, y, model_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        'smote__sampling_strategy': [0.1, 0.25, 0.5, 0.75, 1.0],\n",
    "        'smote__k_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__n_estimators': [100, 200, 300, 400],\n",
    "        'classifier__max_depth': [10, 15, 20, 25],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [2, 4, 6],\n",
    "        'classifier__max_features': [0.3, 0.5, 0.7, 'sqrt', 'log2'],\n",
    "        'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
    "                                       n_iter=50, cv=StratifiedKFold(n_splits=5),\n",
    "                                       scoring='f1', n_jobs=-1, random_state=42)\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Evaluación en conjunto de entrenamiento\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_train_pred_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Evaluación en conjunto de prueba\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nResultados para {model_name}:\")\n",
    "    print(\"\\nMejores hiperparámetros:\")\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    print(\"\\nRendimiento en conjunto de entrenamiento:\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    print(f\"ROC AUC Score (Entrenamiento): {roc_auc_score(y_train, y_train_pred_proba):.4f}\")\n",
    "\n",
    "    print(\"\\nRendimiento en conjunto de prueba:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(f\"ROC AUC Score (Prueba): {roc_auc_score(y_test, y_test_pred_proba):.4f}\")\n",
    "\n",
    "    print(\"\\nMatriz de confusión (Conjunto de prueba):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    joblib.dump(best_model, f'/content/drive/My Drive/TFM_b/exoplanet_model_{model_name}.joblib')\n",
    "    print(f\"Modelo guardado como: exoplanet_model_{model_name}.joblib\")\n",
    "\n",
    "    if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "        plt.title('Importancia de las características')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/content/drive/My Drive/TFM_b/feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Análisis adicional de las características más importantes\n",
    "        top_features = feature_importance_df['feature'].head(5).tolist()\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, feature in enumerate(top_features, 1):\n",
    "            plt.subplot(2, 3, i)\n",
    "            sns.histplot(data=df, x=feature, hue='exoplanet', kde=True)\n",
    "            plt.title(f'Distribución de {feature}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "X = df.drop('exoplanet', axis=1)\n",
    "y = df['exoplanet']\n",
    "feature_importance_df = train_evaluate_model(X, y, \"model6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784c239-a736-4c33-8175-bca1e63d42f9",
   "metadata": {},
   "source": [
    "Resultados para model6:\n",
    "\n",
    "Mejores hiperparámetros:\n",
    "{'smote__sampling_strategy': 0.1, 'smote__k_neighbors': 3, 'classifier__n_estimators': 200, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 4, 'classifier__max_features': 0.7, 'classifier__max_depth': 10, 'classifier__class_weight': None}\n",
    "\n",
    "    Rendimiento en conjunto de entrenamiento:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      1.00      8138\n",
    "           1       0.98      0.81      0.89       349\n",
    "\n",
    "    accuracy                           0.99      8487\n",
    "    macro avg      0.99      0.90      0.94      8487\n",
    "    weighted avg   0.99      0.99      0.99      8487\n",
    "\n",
    "ROC AUC Score (Entrenamiento): 0.9981\n",
    "\n",
    "    Rendimiento en conjunto de prueba:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      0.99      2035\n",
    "           1       0.94      0.74      0.83        87\n",
    "\n",
    "    accuracy                           0.99      2122\n",
    "    macro avg      0.96      0.87      0.91      2122\n",
    "    weighted avg   0.99      0.99      0.99      2122\n",
    "\n",
    "ROC AUC Score (Prueba): 0.9461\n",
    "\n",
    "Matriz de confusión (Conjunto de prueba):<br>\n",
    "[[2031    4]<br>\n",
    " [  23   64]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
